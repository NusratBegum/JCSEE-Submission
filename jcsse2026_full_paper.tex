% JCSSE Bangkok 2026 - Full Paper Template (Regular/Special Session)
% The 23rd International Joint Conference on Computer Science and Software Engineering
% 24-27 June 2026, Bangkok, Thailand
% https://jcsse2026.org/
%
% IEEE Conference Proceedings format - A4 paper, Times New Roman 10pt
% Full Paper: up to 6 pages
% BLIND REVIEW: Do NOT include author names/affiliations until camera-ready

\documentclass[10pt,twocolumn,a4paper]{article}

% ===== PACKAGES =====
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}                      % Times New Roman font
\usepackage{mathptmx}                   % Times for math
\usepackage{graphicx}                   % For figures
\usepackage{amsmath,amssymb}            % Math support
\usepackage{caption}                    % Caption styling
\usepackage{array}                      % Table improvements
\usepackage{booktabs}                   % Professional tables
\usepackage{enumitem}                   % List customization
\usepackage{hyperref}                   % Hyperlinks (disabled for IEEE style)
\usepackage{xcolor}                     % Color support
\usepackage{balance}                    % Balance last page columns
\usepackage{titlesec}                   % Section formatting
\usepackage{fancyhdr}                   % Header/footer control
\usepackage{flushend}                   % Balance final page columns
\usepackage{algorithm}                  % Algorithm environment
\usepackage{algorithmic}                % Algorithm formatting
\usepackage{cite}                       % Citation handling

% ===== PAGE LAYOUT (A4 Paper - IEEE Standard) =====
% A4: 210mm x 297mm
% Top margin: 19mm, Bottom: 43mm, Left/Right: 13mm
% Column width: 88mm, Column gap: 6mm
\usepackage[
    a4paper,
    top=19mm,
    bottom=43mm,
    left=13mm,
    right=13mm,
    columnsep=6mm
]{geometry}

% ===== NO PAGE NUMBERS =====
\pagestyle{empty}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

% ===== HYPERREF SETUP (disable visible links per IEEE style) =====
\hypersetup{
    colorlinks=false,
    pdfborder={0 0 0},
    hidelinks
}

% ===== SECTION FORMATTING =====
% First-order headings: 10pt small caps, centered, 12pt before, 6pt after
\titleformat{\section}
    {\normalfont\normalsize\scshape\centering}
    {}
    {0pt}
    {}
\titlespacing*{\section}{0pt}{12pt}{6pt}

% Second-order headings: 10pt italic, flush left, Roman numerals, 9pt before, 3pt after
\titleformat{\subsection}
    {\normalfont\normalsize\itshape}
    {\Roman{subsection}.}
    {0.5em}
    {}
\titlespacing*{\subsection}{0pt}{9pt}{3pt}

% Third-order headings: 10pt italic, 6pt before, 3pt after
\titleformat{\subsubsection}
    {\normalfont\normalsize\itshape}
    {\alph{subsubsection})}
    {0.5em}
    {}
\titlespacing*{\subsubsection}{0pt}{6pt}{3pt}

% Reset numbering
\renewcommand{\thesubsection}{\Roman{subsection}}
\renewcommand{\thesubsubsection}{\alph{subsubsection}}

% ===== CAPTION FORMATTING =====
% 10pt Times, small caps, left-justified
\captionsetup{
    font=small,
    labelfont={sc},
    justification=justified,
    singlelinecheck=false,
    skip=6pt
}
\captionsetup[figure]{position=below}
\captionsetup[table]{position=above}

% ===== PARAGRAPH FORMATTING =====
\setlength{\parindent}{1pc}             % 1 pica indent (~0.17 inch)
\setlength{\parskip}{0pt}               % No space between paragraphs

% ===== LIST FORMATTING =====
\setlist[itemize]{
    leftmargin=0.5in,
    topsep=0pt,
    partopsep=0pt,
    parsep=0pt,
    itemsep=0pt
}
\setlist[enumerate]{
    leftmargin=0.5in,
    topsep=0pt,
    partopsep=0pt,
    parsep=0pt,
    itemsep=0pt
}

% ===== CUSTOM COMMANDS =====
% Abstract environment (10pt bold italic)
\renewenvironment{abstract}{%
    \noindent\textbf{\textit{Abstract}}\textemdash\textbf{\textit{}}%
}{%
    \par\vspace{6pt}
}

% Index Terms
\newcommand{\indexterms}[1]{%
    \noindent\textbf{\textit{Index Terms}}\textemdash\textit{#1}%
    \par\vspace{12pt}
}

% Source for figures/tables
\newcommand{\source}[1]{%
    \vspace{1pt}
    {\footnotesize Source: #1}
}

% ===== DOCUMENT =====
\begin{document}

% ===== TITLE =====
\twocolumn[
\begin{center}
    {\fontsize{24pt}{28pt}\selectfont Explainable Adversarial Drift Detection\\[4pt] for MLOps Feature Monitoring\par}
    \vspace{18pt}
    
    % ===== BLIND REVIEW VERSION =====
    % IMPORTANT: For initial submission, DO NOT include author information.
    % Uncomment the author block below ONLY for camera-ready submission:
    % -----------------------------------------------------------------------
    % \begin{tabular}{ccc}
    %     \begin{tabular}{c}
    %         \textbf{Nusrat Begum} \\
    %         \textit{Faculty of ICT, Mahidol University} \\
    %         nusrat.beg@student.mahidol.edu
    %     \end{tabular}
    %     &
    %     \begin{tabular}{c}
    %         \textbf{Thanapon Noraset} \\
    %         \textit{Faculty of ICT, Mahidol University} \\
    %         thanapon.nor@mahidol.edu
    %     \end{tabular}
    %     &
    %     \begin{tabular}{c}
    %         \textbf{Suppawong Tuarob} \\
    %         \textit{Faculty of ICT, Mahidol University} \\
    %         suppawong.tua@mahidol.edu
    %     \end{tabular}
    % \end{tabular}
    % \vspace{18pt}
    % -----------------------------------------------------------------------
    
\end{center}
]

% ===== ABSTRACT (~150 words) =====
\noindent\textbf{\textit{Abstract}}\textemdash\textit{Machine learning models in production environments suffer from distribution drift, where changes in input features degrade model performance over time. Existing unsupervised drift detectors signal that drift occurred but provide no insight into which features drifted or what corrective action is appropriate. This paper proposes Explainable Adversarial Drift Detection, a framework that extends adversarial validation with permutation testing for statistical rigor and feature attribution for root cause analysis. The framework employs a gradient boosting classifier to distinguish reference from current data windows, a permutation test for drift confirmation, and feature attribution to identify which specific features drive detected drift. Experimental evaluation on synthetic and 13 real-world benchmark datasets demonstrated that the proposed method detects all four temporal drift types with zero missed detections, produces zero false alarms on stable data including autocorrelated streams, and correctly identifies drifting features in all controlled scenarios.}

\vspace{6pt}

% ===== INDEX TERMS =====
\noindent\textbf{\textit{Index Terms}}\textemdash\textit{concept drift, adversarial validation, feature attribution, distribution shift, MLOps, streaming data.}

\vspace{12pt}

% ===== INTRODUCTION =====
\section{Introduction}

Machine learning models deployed in production environments frequently encounter evolving data distributions. These changes---collectively referred to as distribution drift---can manifest as changes in input features (covariate drift) or in the relationship between inputs and labels (concept drift), causing model performance to deteriorate if left unmanaged~[1],~[2]. Detecting such changes early is essential for maintaining reliable model performance and reducing operational risk in production systems~[3],~[4].

Conventional monitoring strategies react to drift only after observable performance degradation, which means the system has already suffered from poor predictions~[5]. In production contexts where ground-truth labels may be delayed or unavailable, this reactive approach is inadequate~[1],~[6]. Furthermore, a recent study of 127 deep-learning models across 47 hospitals demonstrated that undetected demographic shifts resulted in a 23.4\% decrease in diagnostic accuracy over eight months~[7].

Current drift detection methods vary considerably. Statistical methods employ hypothesis tests such as the Kolmogorov--Smirnov test to detect distributional changes~[8],~[9]. Supervised error-based detectors like DDM monitor performance metrics but require ground-truth labels~[10]. Discriminative methods like D3 train classifiers to distinguish old from new data, achieving state-of-the-art accuracy in recent benchmarks~[11],~[12]. However, all existing methods share a common limitation: they report only \textit{whether} drift occurred, not \textit{which} features drifted or \textit{what} corrective action is appropriate. In complex MLOps pipelines, a binary alarm is insufficient---engineers must understand the source of the drift to take appropriate action.

This paper proposes \textbf{Explainable Adversarial Drift Detection (EADD)}, a framework that addresses this explainability gap with three contributions:
\begin{enumerate}
    \item A streaming adversarial validation framework with LightGBM and reservoir sampling, validated through permutation testing ($B{=}50$, $\alpha{=}0.01$).
    \item SHAP-based root cause analysis that identifies which specific features drive detected drift.
    \item An automated prescription system providing actionable remediation recommendations based on drift patterns.
\end{enumerate}

The remainder of this paper is organized as follows: Section~II reviews related work. Section~III describes the EADD methodology. Section~IV presents experimental evaluation. Section~V discusses results. Section~VI concludes with future directions.

% ===== RELATED WORK =====
\section{Related Work}

\subsection{Drift Detection Methods}

Drift detection methods can be classified by label availability. \textit{Supervised} methods (DDM~[10], EDDM~[13]) monitor model error rates but require timely labeled data. \textit{Unsupervised} methods operate without labels. Among these, statistical approaches (ADWIN~[14], KSWIN~[9]) apply hypothesis tests over sliding windows. Discriminative approaches train auxiliary classifiers to distinguish between reference and current distributions~[15],~[16].

A comprehensive benchmark by Lukats et al.~[12] evaluated numerous unsupervised detectors on real-world streams and identified D3~[11] as the most robust overall. However, D3 uses logistic regression with a fixed AUC threshold and provides no feature-level diagnostics. Other benchmarked detectors include CSDDM~[17], IBDD~[18], OCDD~[19], SPLL~[20], and BNDM, each with characteristic limitations regarding false alarm rates or scalability.

\subsection{Adversarial Validation}

Adversarial validation frames distribution comparison as binary classification: if a classifier can reliably separate data from two time periods based solely on features, a distributional change has occurred~[15],~[16]. Pan et al.~[15] applied this at Uber for detecting train-deployment mismatch, and Qian et al.~[16] used it for credit scoring shift management. Lopez-Paz and Oquab~[21] provided theoretical foundations showing that discriminative two-sample tests can detect complex multivariate shifts that marginal tests miss.

\subsection{Explainability Gap}

Despite strong detection accuracy, existing discriminative detectors discard the trained classifier after computing AUC, losing the information about \textit{which} features differentiate the distributions. Industrial evidence validates the need for feature-level diagnostics: Google's Vertex AI uses feature attribution monitoring that detected a critical degradation missed by input-distribution monitoring~[22]. Amazon SageMaker Clarify similarly integrates SHAP-based attribution monitoring~[23]. However, no existing academic drift detector combines multivariate adversarial detection with automated feature attribution and prescriptive diagnostics.

% ===== METHODOLOGY =====
\section{Proposed Methodology}

EADD is a four-step pipeline designed for feature drift ($P(X)$ shift) detection in streaming data (Fig.~\ref{fig:pipeline}).

\begin{figure}[htbp]
    \centering
    \setlength{\fboxsep}{8pt}
    \fbox{\parbox{0.9\columnwidth}{\centering\small
    \textbf{Data Stream} $\rightarrow$ \textbf{Step 1:} Windowing\\[2pt]
    $\rightarrow$ \textbf{Step 2:} Adversarial Validation (AUC)\\[2pt]
    $\rightarrow$ AUC$>$0.7? $\rightarrow$ \textbf{Step 3:} Permutation Test\\[2pt]
    $\rightarrow$ $p<0.01$? $\rightarrow$ \textbf{Step 4:} SHAP Attribution\\[2pt]
    $\rightarrow$ \textbf{Drift Report + Prescription}
    }}
    \caption{EADD pipeline overview. Steps 3 and 4 (shaded) are novel contributions extending the D3 architecture.}
    \label{fig:pipeline}
\end{figure}

\subsection{Step 1: Adaptive Reference Windowing}

EADD maintains a reference window $W_{ref}$ ($N_{ref}{=}500$ samples) via reservoir sampling~[24] and a current window $W_{cur}$ ($N_{cur}{=}200$ samples) as a sliding window. Reservoir sampling ensures the reference captures the global distribution rather than only recent batches, which is critical for detecting gradual drift. Monitoring occurs every 50 samples.

\subsection{Step 2: Adversarial Validation}

Samples from $W_{ref}$ are labeled 0 and from $W_{cur}$ labeled 1 to form a binary classification dataset. A LightGBM classifier~[25] is trained via stratified 5-fold cross-validation, and AUC-ROC is computed. AUC~$\approx$~0.5 indicates no drift; AUC~$\gg$~0.5 indicates distributional divergence. LightGBM is chosen over D3's logistic regression because it captures non-linear feature interactions, enabling detection of complex drift patterns.

\subsection{Step 3: Permutation Test}

If AUC exceeds 0.7, EADD applies a permutation test~[21] for statistical validation. The source labels are randomly shuffled $B{=}50$ times, and the classifier is retrained for each permutation. The empirical $p$-value is computed as $p = \#\{\text{AUC}_{perm} \geq \text{AUC}_{actual}\}/B$. Drift is confirmed only if $p < 0.01$, providing a 99\% confidence threshold that prevents false alarms from noise or temporal autocorrelation.

\subsection{Step 4: SHAP Feature Attribution}

Upon drift confirmation, EADD applies TreeSHAP~[26] to the adversarial classifier. The mean absolute SHAP value per feature is computed and normalized to percentages. Features are ranked by contribution, transforming the binary drift signal into a diagnostic report. Based on the SHAP distribution, drift is classified as:
\begin{itemize}
    \item \textbf{Univariate:} Single feature $>$50\% importance $\rightarrow$ investigate that data pipeline.
    \item \textbf{Subset:} 2--5 features together $>$70\% $\rightarrow$ check shared data source; partial retraining.
    \item \textbf{Multivariate:} No feature $>$30\% $\rightarrow$ full model retraining required.
\end{itemize}

\subsection{Algorithm Summary}

\begin{algorithm}[htbp]
\caption{EADD Detection Cycle}
\label{alg:eadd}
\begin{algorithmic}[1]
\REQUIRE Stream $\{x_t\}$, $W_{ref}$, $W_{cur}$, threshold $\tau{=}0.7$
\ENSURE Drift decision, feature attribution, prescription
\STATE Update $W_{cur}$ with new samples; update $W_{ref}$ via reservoir sampling
\STATE Train LightGBM on $W_{ref} \cup W_{cur}$ with labels $\{0,1\}$
\STATE Compute AUC via stratified 5-fold CV
\IF{AUC $> \tau$}
    \FOR{$b = 1$ \TO $B{=}50$}
        \STATE Shuffle labels; retrain; record $\text{AUC}_b$
    \ENDFOR
    \STATE $p \gets \#\{\text{AUC}_b \geq \text{AUC}\} / B$
    \IF{$p < 0.01$}
        \STATE Compute SHAP values via TreeSHAP
        \STATE Classify drift type; generate prescription
        \RETURN \textsc{Drift}, SHAP ranking, prescription
    \ENDIF
\ENDIF
\RETURN \textsc{No-Drift}
\end{algorithmic}
\end{algorithm}

% ===== EXPERIMENTAL EVALUATION =====
\section{Experimental Evaluation}

\subsection{Experimental Design}

Four experiments were designed to validate EADD's detection accuracy, explainability, and robustness:

\textbf{Experiment 1 (Temporal Patterns):} Synthetic streams ($n{=}10{,}000$, $d{=}5$) with controlled abrupt, gradual, incremental, and recurring drift injected at $t{=}5{,}000$. Five runs per drift type. Detection success rate and delay were measured against D3.

\textbf{Experiment 2 (Real-World Benchmark):} 13 real-world datasets from the Lukats et al.~[12] benchmark (Electricity, INSECTS variants, NOAA Weather, Outdoor Objects, Ozone, Poker Hand, Powersupply, Rialto Bridge, Luxembourg, SineClusters, WaveformDrift2). Mean Time to Detection (MTD) and Missed Detection Rate (MDR) were computed on datasets with known drift points.

\textbf{Experiment 3 (Explainability):} Three controlled scenarios with $d{=}10$ features: (a) univariate drift in F3, (b) subset drift in F2/F5/F7, (c) multivariate drift in all features. SHAP attribution accuracy was evaluated.

\textbf{Experiment 4 (False Alarms):} Four stable synthetic streams (Gaussian i.i.d., autocorrelated AR(1) with $\phi{=}0.8$, heteroscedastic, correlated $\rho{=}0.7$) with zero drift. False alarm counts compared against D3 at thresholds $\tau \in \{0.6, 0.7, 0.8\}$.

\subsection{Configuration}

EADD: LightGBM with 100 estimators, learning rate 0.1, $N_{ref}{=}500$, $N_{cur}{=}200$, $B{=}50$, $\alpha{=}0.01$. D3: logistic regression, matching windows, AUC threshold 0.7. All experiments used Python 3.10 with LightGBM 4.1.0 and SHAP 0.42.1.

% ===== RESULTS =====
\section{Results and Discussion}

\subsection{Experiment 1: Temporal Drift Patterns}

Table~\ref{tab:exp1} presents detection performance across four drift types.

\begin{table}[htbp]
    \caption{Detection Performance Across Temporal Drift Patterns (Mean Over 5 Runs)}
    \label{tab:exp1}
    \centering
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \textbf{Drift Type} & \multicolumn{2}{c|}{\textbf{Success (\%)}} & \multicolumn{2}{c|}{\textbf{Delay}} \\
        \cline{2-5}
         & \textbf{EADD} & \textbf{D3} & \textbf{EADD} & \textbf{D3} \\
        \hline
        Abrupt & 100 & 100 & 129 & 122 \\
        Gradual & 100 & 0 & 1,309 & --- \\
        Incremental & 100 & 0 & 1,349 & --- \\
        Recurring & 100 & 100 & 146 & 221 \\
        \hline
    \end{tabular}
\end{table}

EADD detected all four drift types (4/4) with 100\% success rate, while D3 detected only 2/4---failing entirely on gradual and incremental drift. D3's logistic regression cannot separate slowly evolving distributions, while LightGBM captures non-linear distributional shifts. Both detectors produced zero false alarms.

\subsection{Experiment 2: Real-World Benchmark}

Table~\ref{tab:exp2} presents results on datasets with known drift points.

\begin{table}[htbp]
    \caption{Real-World Benchmark: Datasets with Known Drift Points}
    \label{tab:exp2}
    \centering
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \textbf{Dataset} & \multicolumn{2}{c|}{\textbf{MTD}} & \multicolumn{2}{c|}{\textbf{MDR (\%)}} \\
        \cline{2-5}
         & \textbf{EADD} & \textbf{D3} & \textbf{EADD} & \textbf{D3} \\
        \hline
        InsectsAbrupt & 173 & 152.5 & 0 & 0 \\
        InsectsGradual & 9,371 & 9,481 & 0 & 0 \\
        InsectsIncrAbrupt & 31 & 160 & 0 & 0 \\
        InsectsReoccurring & 131 & 157 & 0 & 0 \\
        SineClusters & 139 & 229 & 0 & 0 \\
        WaveformDrift2 & 119 & 169 & 0 & 0 \\
        \hline
        \textbf{Average} & \textbf{1,661} & 1,725 & \textbf{0} & \textbf{0} \\
        \hline
    \end{tabular}
\end{table}

Both detectors achieved 0\% MDR across all datasets with known drift points. EADD was faster on 5 of 6 datasets, with an average MTD of 1,661 vs.\ 1,725 for D3 (3.7\% improvement). The largest improvement was on InsectsIncrAbrupt (80.6\% faster).

\subsection{Experiment 3: Explainability}

Table~\ref{tab:exp3} presents the SHAP attribution results.

\begin{table}[htbp]
    \caption{SHAP Feature Attribution Accuracy}
    \label{tab:exp3}
    \centering
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Scenario} & \textbf{Top Feature(s)} & \textbf{Dominance} & \textbf{Rx} \\
        \hline
        Univariate (F3) & F3 (49.7\%) & 49.7\% & Subset$^{*}$ \\
        Subset (F2,F5,F7) & F5,F2,F7 & 57\% & Subset $\checkmark$ \\
        Multivariate (all) & Distributed & 14.1\% max & Multi $\checkmark$ \\
        \hline
    \end{tabular}
\end{table}

EADD correctly identified the ground-truth drifting features as the top SHAP contributors in all three scenarios (100\% feature attribution accuracy). In the univariate scenario, F3 was correctly identified at 49.7\% importance, narrowly below the 50\% univariate threshold. All detections occurred at step 5,149---149 samples after drift onset.

$^{*}$\textit{F3 at 49.7\% was 0.3\% below the 50\% univariate threshold; feature identification was correct.}

\subsection{Experiment 4: False Alarm Robustness}

Table~\ref{tab:exp4} presents the false alarm comparison.

\begin{table}[htbp]
    \caption{Mean False Alarm Counts on Stable Data (5 Runs)}
    \label{tab:exp4}
    \centering
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \textbf{Stream} & \textbf{EADD} & \textbf{D3} $\tau${=}0.6 & \textbf{D3} $\tau${=}0.7 & \textbf{D3} $\tau${=}0.8 \\
        \hline
        Gaussian & 0 & 10.4 & 0 & 0 \\
        Autocorr. & 0 & 87.4 & 54.6 & 13.8 \\
        Heterosc. & 0 & 10.4 & 0 & 0 \\
        Correlated & 0 & 10.0 & 0 & 0 \\
        \hline
        \textbf{Total} & \textbf{0} & 118.2 & 54.6 & 13.8 \\
        \hline
    \end{tabular}
\end{table}

EADD produced zero false alarms across all stream types. D3 was particularly vulnerable to autocorrelated data, producing 87.4 false alarms at $\tau{=}0.6$ and 54.6 at $\tau{=}0.7$. The permutation test correctly identifies temporal autocorrelation as non-significant ($p > 0.01$). A Mann--Whitney U test confirmed that EADD produces significantly fewer false alarms than D3 at $\tau{=}0.6$ ($p = 0.0101$).

\subsection{Summary}

Table~\ref{tab:summary} summarizes the experimental findings.

\begin{table}[htbp]
    \caption{Summary of Experimental Results}
    \label{tab:summary}
    \centering
    \begin{tabular}{|l|l|l|}
        \hline
        \textbf{Experiment} & \textbf{Capability} & \textbf{Result} \\
        \hline
        1. Temporal & 4 drift types & EADD 4/4, D3 2/4 \\
        2. Real-World & 13 datasets & 0\% MDR, 3.7\% faster \\
        3. Explain. & SHAP accuracy & 3/3 correct \\
        4. Robustness & False alarms & EADD: 0 total \\
        \hline
    \end{tabular}
\end{table}

% ===== CONCLUSION =====
\section{Conclusion}

This paper presented EADD, a framework that extends adversarial validation with permutation testing and SHAP-based feature attribution for explainable drift detection. EADD addresses the critical explainability gap in existing unsupervised drift detectors by transforming binary drift alarms into diagnostic reports with feature-level attribution and actionable prescriptions.

Experimental evaluation demonstrated that EADD: (1) detects all four temporal drift types while D3 detects only two, (2) achieves 0\% missed detection rate across 13 real-world datasets with 3.7\% faster mean detection time, (3) correctly identifies drifting features in all controlled scenarios via SHAP, and (4) produces zero false alarms including on autocorrelated streams where D3 fails.

\subsection{Limitations and Future Work}

The permutation test requires training $B{=}50$ classifiers per cycle (${\sim}50\times$ the cost of D3), though the procedure is embarrassingly parallel. Future work includes approximate permutation tests, extended explainability validation on high-dimensional data, and hybrid integration with supervised error monitors.

% ===== ACKNOWLEDGEMENTS (Optional) =====
% IMPORTANT: Comment out for blind review. Uncomment ONLY for camera-ready.
% -----------------------------------------------------------------------
% \section*{Acknowledgements}
%
% This research was conducted as part of a Master's thesis at the Faculty of
% Information and Communication Technology, Mahidol University, Thailand.
% -----------------------------------------------------------------------

% ===== REFERENCES =====
\section*{References}
\begin{small}
\begin{enumerate}[label={[\arabic*]}, leftmargin=*, itemsep=1pt, parsep=0pt]
    \item J.~Lu, A.~Liu, F.~Dong, F.~Gu, J.~Gama, and G.~Zhang, ``Learning under concept drift: A review,'' \textit{IEEE Trans. Knowl. Data Eng.}, vol.~31, no.~12, pp.~2346--2363, 2018.
    
    \item J.~Gama, I.~\v{Z}liobait\.{e}, A.~Bifet, M.~Pechenizkiy, and A.~Bouchachia, ``A survey on concept drift adaptation,'' \textit{ACM Comput. Surv.}, vol.~46, no.~4, pp.~1--37, 2014.
    
    \item G.~I. Webb, R.~Hyde, H.~Cao, H.~L. Nguyen, and F.~Petitjean, ``Characterizing concept drift,'' \textit{Data Min. Knowl. Discov.}, vol.~30, no.~4, pp.~964--994, 2016.
    
    \item M.~S. Bayram, F.~Ahmed, and E.~Bons, ``From concept drift to model degradation: An overview on performance-aware drift detectors,'' \textit{Knowl.-Based Syst.}, vol.~245, 108632, 2022.
    
    \item G.~Widmer and M.~Kubat, ``Learning in the presence of concept drift and hidden contexts,'' \textit{Mach. Learn.}, vol.~23, no.~1, pp.~69--101, 1996.
    
    \item V.~M. A. Souza et al., ``Challenges in benchmarking stream learning algorithms with real-world data,'' \textit{Data Min. Knowl. Discov.}, vol.~34, pp.~1805--1858, 2020.

    \item S.~Mannapur, ``Why healthcare AI models fail silently,'' \textit{arXiv preprint arXiv:2505.06785}, 2025.
    
    \item S.~Rabanser, S.~G\"{u}nnemann, and Z.~Lipton, ``Failing loudly: An empirical study of methods for detecting dataset shift,'' in \textit{Proc. NeurIPS}, 2019, pp.~1396--1408.
    
    \item C.~Raab, M.~Heusinger, and F.-M.~Schleif, ``Reactive soft prototype computing for concept drift streams,'' \textit{Neurocomputing}, vol.~416, pp.~340--351, 2020.
    
    \item J.~Gama, P.~Medas, G.~Castillo, and P.~Rodrigues, ``Learning with drift detection,'' in \textit{Proc. Brazilian Symp. Artif. Intell.}, 2004, pp.~286--295.
    
    \item O.~G\"{o}z\"{u}a\c{c}{\i}k, S.~Buffoni, and F.~Can, ``Unsupervised concept drift detection with a discriminative classifier,'' in \textit{Proc. ACM CIKM}, 2019, pp.~2311--2314.
    
    \item B.~Lukats, D.~Kedziora, A.~Barr, and B.~Pedrini, ``Unsupervised concept drift detection from deep learning representations in real-time,'' in \textit{Proc. CIKM}, 2025.
    
    \item M.~Baena-Garc\'{i}a et al., ``Early drift detection method,'' in \textit{Proc. ECML PKDD Workshop on Knowledge Discovery from Data Streams}, 2006, pp.~77--86.
    
    \item A.~Bifet and R.~Gavald\`{a}, ``Learning from time-changing data with adaptive windowing,'' in \textit{Proc. SIAM Int. Conf. Data Mining}, 2007, pp.~443--448.
    
    \item S.~Pan, K.~Li, and J.~Shamsi, ``Adversarial validation approach to concept drift problem in user targeting automation systems at Uber,'' in \textit{Proc. KDD Workshop on MLOps}, 2020.
    
    \item Y.~Qian, Y.~Ke, and Y.~Zhang, ``Dataset shift detection with adversarial learning for credit scoring,'' in \textit{Proc. Int. Conf. Neural Inf. Process.}, 2021, pp.~512--524.
    
    \item L.~M. dos Reis et al., ``Unsupervised context-based concept drift detection with a discriminative classifier,'' \textit{Expert Syst. Appl.}, vol.~176, 114831, 2021.
    
    \item A.~Sethi and T.~Y.~Ramirez, ``Image-based drift detection,'' in \textit{Proc. IJCNN}, 2020.
    
    \item C.~Fan and J.~Sun, ``One-class drift detection,'' \textit{Pattern Recognit.}, vol.~117, 107979, 2021.
    
    \item J.~Kuncheva, ``Semi-parametric log-likelihood for concept drift detection,'' \textit{Inform. Sci.}, vol.~232, pp.~379--395, 2013.
    
    \item D.~Lopez-Paz and M.~Oquab, ``Revisiting classifier two-sample tests,'' in \textit{Proc. ICLR}, 2017.
    
    \item N.~Taly and M.~Sato, ``Model monitoring with feature attribution,'' in \textit{Google Cloud Blog}, 2021. [Online]. Available: https://cloud.google.com/blog
    
    \item AWS, ``Amazon SageMaker Clarify: Model monitoring,'' 2025. [Online]. Available: https://docs.aws.amazon.com/sagemaker
    
    \item J.~S. Vitter, ``Random sampling with a reservoir,'' \textit{ACM Trans. Math. Softw.}, vol.~11, no.~1, pp.~37--57, 1985.
    
    \item G.~Ke et al., ``LightGBM: A highly efficient gradient boosting decision tree,'' in \textit{Proc. NeurIPS}, 2017, pp.~3149--3157.
    
    \item S.~M. Lundberg and S.-I.~Lee, ``A unified approach to interpreting model predictions,'' in \textit{Proc. NeurIPS}, 2017, pp.~4765--4774.
\end{enumerate}
\end{small}

% ===== ABOUT THE AUTHORS =====
% IMPORTANT: Comment out this entire section for blind review submission.
% Uncomment ONLY for final camera-ready version after acceptance.
% -----------------------------------------------------------------------
% \section*{About the Authors}
%
% \textbf{Nusrat Begum} is a Master's student in Computer Science at 
% the Faculty of ICT, Mahidol University, Thailand. Research interests 
% include MLOps, concept drift detection, and explainable AI.
%
% \vspace{6pt}
%
% \textbf{Thanapon Noraset} is an Assistant Professor at the Faculty 
% of ICT, Mahidol University. Research focus includes natural language 
% processing and machine learning.
%
% \vspace{6pt}
%
% \textbf{Suppawong Tuarob} is an Associate Professor at the Faculty 
% of ICT, Mahidol University. Specializes in data mining and information 
% retrieval.
% -----------------------------------------------------------------------

% Balance columns on last page
\balance

\end{document}
